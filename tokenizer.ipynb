{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## This notebook will cover the basic concept of tokenizer, how they work with data. The basic definition of tokenizer is:\n",
    "## A tokenizer is a program that convert a sequence of characters into a sequence of tokens."
   ],
   "id": "c65c169cc03faad7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T10:48:25.632428Z",
     "start_time": "2026-02-06T10:48:25.596745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's start with examples\n",
    "text = open('file.txt', 'r').read()\n",
    "words = text.split(\" \")\n",
    "tokens = {v: k for k, v in enumerate(words)}\n",
    "tokens\n",
    "# This is the basic code of tokenization. You can tokenize the sentence using this code, but it's too slow for huge text file. So, for fast tokenization, we use libraries for tokenization"
   ],
   "id": "1aebfd81f6884108",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': 0,\n",
       " 'is': 1,\n",
       " 'the': 2,\n",
       " 'txt': 3,\n",
       " 'file': 4,\n",
       " 'that': 5,\n",
       " 'will': 6,\n",
       " 'use': 7,\n",
       " 'for': 8,\n",
       " 'tokenization.': 9}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tokenization Overview\n",
    "\n",
    "#### A tokenizer (Hugging Face Library) converts raw text into smaller units called tokens so that machines can process language.\n",
    "\n",
    "#### Instead of using full words, modern NLP systems often use subword tokenization to handle unknown or rare words effectively.\n",
    "\n",
    "## Subword Tokenization Methods\n",
    "\n",
    "### 1. Byte Pair Encoding (BPE):\n",
    "\n",
    "#### breaks words into frequent character pairs by repeatedly merging the most common sequences. It is widely used in GPT-style models.\n",
    "\n",
    "#### For example: texts are: low, lower, lowest. Most common pair = l + o → merge → lo Then lo + w → low.\n",
    "\n",
    "#### Final tokens might be: low, low + er, low + est. Simple rule: BPE learns tokens based on frequency.\n",
    "\n",
    "#### Use in: GPT model, RoBERTa, many production systems\n",
    "\n",
    "### 2. WordPiece:\n",
    "\n",
    "#### is similar to BPE but selects subwords based on probabilistic usefulness rather than only frequency. It is used in BERT-based models.\n",
    "\n",
    "#### For example: texts are playing → play + ##ing. ##ing means: “this piece comes after another piece”. WP learns tokens based on probability.\n",
    "\n",
    "#### Used in: BERT, DistilBERT\n",
    "\n",
    "### 3. SentencePiece:\n",
    "\n",
    "#### treats text as a sequence of characters without relying on spaces, making it suitable for non-space-based languages.  It is used in models like T5 and LLaMA.\n",
    "\n",
    "#### It is mostly use for languages like chinese, urdu, spanish etc. Simple rule: SentencePiece works directly on raw text, spaces included.\n",
    "\n",
    "#### Used in: T5, ALBERT, mBERT, LLaMA\n",
    "\n",
    "## Industry Usage\n",
    "\n",
    "#### 1. Transformers / LLMs → Hugging Face tokenizers\n",
    "\n",
    "#### 2. Classical NLP pipelines → spaCy tokenizer"
   ],
   "id": "eef81e01f7b69e17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T12:12:46.769395Z",
     "start_time": "2026-02-06T12:12:46.766349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We will use tokenizer here\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE"
   ],
   "id": "b1519743fd2a8b13",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T12:12:47.089209Z",
     "start_time": "2026-02-06T12:12:47.085968Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))",
   "id": "d323f4f8935902f4",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T12:12:47.300345Z",
     "start_time": "2026-02-06T12:12:47.296805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# If you want to train a new tokenizer just like above UNK from scratch, use BPE trainer\n",
    "# Here, we are just adding new tokens like unk (unknown), cls (classification) etc and add some rule in bpe.\n",
    "\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    ")"
   ],
   "id": "5c61c29a940a385c",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T12:14:45.629288Z",
     "start_time": "2026-02-06T12:14:45.626205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "files = [\n",
    "    f\"wikitext-103/wiki.{split}.tokens\"\n",
    "    for split in [\"train\", \"test\", \"valid\"]\n",
    "]\n"
   ],
   "id": "c22fa3344762ad07",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T13:43:18.746706Z",
     "start_time": "2026-02-06T12:14:46.341558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NOTE: Here, train means add new tokens to dataset, not train model.\n",
    "# WARNING: This code section will take huge time like 1 hour, or 1.5 hour.\n",
    "tokenizer.train(files, trainer)"
   ],
   "id": "b32ed80b52821ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "## Tokenization Guidelines\n",
    "\n",
    "### Before creating embeddings, text data must be tokenized. How you do this depends on the type of data:\n",
    "\n",
    "### 1. Sensitive data (legal, finance, medical reports, etc.):\n",
    "\n",
    "#### Use a custom tokenizer and train it with methods like SentencePiece, WordPiece, or BPE.\n",
    "#### ⚠️ This may take a significant amount (like 1.5 hour) of time, but it ensures better handling of sensitive or domain-specific vocabulary.\n",
    "\n",
    "### 2. Non-sensitive or general data:\n",
    "\n",
    "#### You can use pre-built tokenizers from Hugging Face or spaCy.\n",
    "\n",
    "#### ✅ This saves processing time and works well for most common datasets."
   ],
   "id": "9e737f21db66314f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "668b81b413819b97"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
